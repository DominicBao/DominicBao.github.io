<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>spark的学习笔记（1） | 阿方的博客</title>
<link rel="shortcut icon" href="https://dominicbao.github.io/favicon.ico?v=1633275398441">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://dominicbao.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="spark的学习笔记（1） | 阿方的博客 - Atom Feed" href="https://dominicbao.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="
2019-11-11 @阿方

什么是spark
用于大规模数据处理的统一分析引擎，实现快速通用的集群计算平台，是通用内存并行计算框架，用来构建大型的，低延迟的数据分析应用程序。Spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘..." />
    <meta name="keywords" content="spark,教程" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://dominicbao.github.io">
  <img class="avatar" src="https://dominicbao.github.io/images/avatar.png?v=1633275398441" alt="">
  </a>
  <h1 class="site-title">
    阿方的博客
  </h1>
  <p class="site-description">
    快乐学习，美好生活
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/DominicBao" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              spark的学习笔记（1）
            </h2>
            <div class="post-info">
              <span>
                2021-04-26
              </span>
              <span>
                9 min read
              </span>
              
                <a href="https://dominicbao.github.io/tag/DCZloVJH_/" class="post-tag">
                  # spark
                </a>
              
                <a href="https://dominicbao.github.io/tag/aQaAeaeCj/" class="post-tag">
                  # 教程
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <blockquote>
<p>2019-11-11 @阿方</p>
</blockquote>
<h3 id="什么是spark">什么是spark</h3>
<p>用于大规模数据处理的统一分析引擎，实现快速通用的集群计算平台，是通用内存并行计算框架，用来构建大型的，低延迟的数据分析应用程序。Spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算</p>
<h3 id="spark的四大特性">Spark的四大特性</h3>
<ul>
<li>高效性<br>
用嘴先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。</li>
<li>易用性<br>
支持java，python，Scala的API，支持超过80种高级算法。支持交互式的Python和Scala的shell，可以方便的在这些shell中使用Spark集群解决问题</li>
<li>通用性<br>
提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用<br>
<img src="https://dominicbao.github.io/post-images/1619447115609.png" alt="" loading="lazy"></li>
<li>兼容性<br>
字面意思，兼容性高</li>
</ul>
<h3 id="spark的组成">Spark的组成：</h3>
<p>他的主要组成有</p>
<ul>
<li>SparkCore：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API</li>
<li>SparkSQL：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容</li>
<li>SparkStreaming： 是Spark提供的实时数据进行流式计算的组件</li>
<li>MLlib：提供常用机器学习算法的实现库</li>
<li>GraphX：提供一个分布式图计算框架，能高效进行图计算</li>
<li>BlinkDB：用于在海量数据上进行交互式SQL的近似查询引擎</li>
<li>Tachyon：以内存为中心高容错的的分布式文件系统</li>
</ul>
<h3 id="本地部署spark">本地部署spark</h3>
<p>在公司的dc平台上我们使用的是pyspark但由于spark是由scala编写的，所以使用scala会更有利于我们更好的理解spark引擎，所以这里我要部署在本地的是scala为api的spark</p>
<ol>
<li>
<p>下载编译工具，在这之前我一直使用的是eclipse，但由于网上众多网友都吹intelliJ IDEA有多好，所以这里我也使用idea来进行部署，首先在官方网站上下载，这里可以下载免费的社区版应该功能够了，我使用的是旗舰版因为学生任性</p>
</li>
<li>
<p>下载后进行安装，点点点后会到这个操作界面，我们点击右下角如红色圈圈锁住额地方，下载scala插件：<br>
<img src="https://dominicbao.github.io/post-images/1619447314137.png" alt="" loading="lazy"><br>
这里我已经下载好了，你们就搜scala然后下载就好<br>
<img src="https://dominicbao.github.io/post-images/1619447368873.png" alt="" loading="lazy"></p>
</li>
<li>
<p>然后我们重启idea后再点击如下图右下角红色圈圈的地方<br>
<img src="https://dominicbao.github.io/post-images/1619447408196.png" alt="" loading="lazy"><br>
在这里我们需要配置两个东西，一个是jdk一个是scala，这里要注意，由于这里spark版本我选择的是1.4.4（dc平台上是1.4），由于兼容性的问题，jdk需要8+，scala需要1.2.x，不能1.3不能1.1，详情可以看spark官方网站上的兼容性的文档<br>
https://spark.apache.org/docs/latest/index.html<br>
首先是jdk的包，一般来讲下载idea会自带的下两个jdk，如果没有的需要你自行去下载并且配置好全局变量：<br>
<img src="https://dominicbao.github.io/post-images/1619447488600.png" alt="" loading="lazy"><br>
然后下载scala，一般来讲是没有下载过的，但如果你自己下载了就直接导入就好，没有的话按如下图三个地方，记得版本要选1.2.x的不能太高也不能太低<br>
<img src="https://dominicbao.github.io/post-images/1619447520778.png" alt="" loading="lazy"><br>
记得先点apply再点ok</p>
</li>
<li>
<p>然后就可以创建一个新的项目了，这里为了使配置更加简单一点，我们采用maven项目，上面红圈内选择自己的jdk版本,应该会自动显示出来，如果没有的话自己选一下，然后点击next：<br>
<img src="https://dominicbao.github.io/post-images/1619448192256.png" alt="" loading="lazy"><br>
接下去就是maven的基本配置了，自己随便取个名字然后next，finally二连就可以了<br>
<img src="https://dominicbao.github.io/post-images/1619448219639.png" alt="" loading="lazy"><br>
<img src="https://dominicbao.github.io/post-images/1619448226912.png" alt="" loading="lazy"><br>
如果你是第一次创建maven项目的话第一次编译可能需要一点时间，等他编译好就好，一般来讲不会出现什么问题，如果有，算你脸黑自行百度哈</p>
</li>
<li>
<p>编译完成后的样子应该是这样的：<br>
<img src="https://dominicbao.github.io/post-images/1619448258764.png" alt="" loading="lazy"><br>
其中红框的地方你应该没有，那是build后产生的calss文件，然后我们首先配置scala环境，右击主标题，导入其他的包，如下图所示<br>
<img src="https://dominicbao.github.io/post-images/1619448282424.png" alt="" loading="lazy"><br>
导入scala的包，在这个界面找到scala前面打上勾ok就好，这里因为我已经导入了看不见了所以就不模拟了<br>
<img src="https://dominicbao.github.io/post-images/1619448306166.png" alt="" loading="lazy"><br>
在src-main，创建scala文件夹<br>
<img src="https://dominicbao.github.io/post-images/1619448332116.png" alt="" loading="lazy"><br>
右击它，将它设置为root文件夹，正常来讲你会发现它的颜色变成了和上面的java文件夹一样<br>
<img src="https://dominicbao.github.io/post-images/1619448355962.png" alt="" loading="lazy"><br>
这里由于我已经将其设为了root文件夹所以只有取消的按钮，不掩饰了哈，这时候你可以创建一个scala任务跑一下验证你的scala配置是否出错<br>
<img src="https://dominicbao.github.io/post-images/1619448384637.png" alt="" loading="lazy"><br>
在object中输出一下的代码，启动看看是否会输出hello word<br>
<img src="https://dominicbao.github.io/post-images/1619448418166.png" alt="" loading="lazy"></p>
</li>
<li>
<p>最后就是配置maven的pom.xml文件进行spark包的导入了，由于是maven嘛，你懂的，在pom.xml文件中加入如下的语句，我这里的版本如下spark（1.4.4），jdk（12），scala（1.2.8）</p>
</li>
</ol>
<pre><code class="language-xml">&lt;properties&gt;
        &lt;spark.version&gt;2.4.4&lt;/spark.version&gt;
        &lt;scala.version&gt;2.12&lt;/scala.version&gt;
    &lt;/properties&gt;


&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_${scala.version}&lt;/artifactId&gt;
        &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_${scala.version}&lt;/artifactId&gt;
        &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-sql_${scala.version}&lt;/artifactId&gt;
        &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-hive_${scala.version}&lt;/artifactId&gt;
        &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-mllib_${scala.version}&lt;/artifactId&gt;
        &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;

&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
            &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.15.2&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;goal&gt;testCompile&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.6.0&lt;/version&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.8&lt;/source&gt;
                &lt;target&gt;1.8&lt;/target&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.19&lt;/version&gt;
            &lt;configuration&gt;
                &lt;skip&gt;true&lt;/skip&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;

    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>
<p>然后静静等他把该配的包配完就好了，完成后在刚刚创建的scala文件中输入以下的代码，运行完成就ok了，如果这里报错了，自行百度哈，你找我我也是百度谷歌给你看滴</p>
<pre><code class="language-java">import org.apache.spark.{SparkConf, SparkContext}

object myFiistScala {
  def main(args: Array[String]): Unit = {
    println(&quot;Hello world&quot;)
    val conf = new SparkConf().setAppName(&quot;mySpark&quot;)
    //setMaster(&quot;local&quot;) 本机的spark就用local，远端的就写ip
    //如果是打成jar包运行则需要去掉 setMaster(&quot;local&quot;)因为在参数中会指定。
    conf.setMaster(&quot;local&quot;)
    val sc =new SparkContext(conf)
    val rdd =sc.parallelize(List(1,2,3,4,5,6)).map(_*3)
    val mappedRDD=rdd.filter(_&gt;10).collect()
    //对集合求和
    println(rdd.reduce(_+_))
    //输出大于10的元素
    for(arg &lt;- mappedRDD)
      print(arg+&quot; &quot;)
    println()
    println(&quot;math is work&quot;)
  }
}
</code></pre>
<p>最后展示一下成功的样子趴，祝大家都可以跟我一样哈，last，如果以上有啥专业术语看不懂，强烈建议去补下课哈<br>
<img src="https://dominicbao.github.io/post-images/1619448802495.png" alt="" loading="lazy"></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AFspark">什么是spark</a></li>
<li><a href="#spark%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7">Spark的四大特性</a></li>
<li><a href="#spark%E7%9A%84%E7%BB%84%E6%88%90">Spark的组成：</a></li>
<li><a href="#%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2spark">本地部署spark</a></li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://dominicbao.github.io/post/20190826-01/">
              <h3 class="post-title">
                Python内存管理与垃圾回收
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '6618716d54229d617646',
    clientSecret: '9eefcf8c0924139abd6befd51e19a6764e6e1732',
    repo: 'DominicBao.github.io',
    owner: 'DominicBao',
    admin: ['DominicBao'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  阿方工作室
  <a class="rss" href="https://dominicbao.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
